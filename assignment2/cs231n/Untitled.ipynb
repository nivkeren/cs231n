{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 2, None, 3, None, 4, None, 5, None, 6]\n",
      "{'W5': 10, 'b5': 10, 'W4': 8, 'b4': 8, 'W3': 6, 'b3': 6, 'W2': 4, 'b2': 4, 'W1': 2, 'b1': 2}\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "n = 5\n",
    "num_loop = 2\n",
    "num_all_layers = n * num_loop + 1\n",
    "outs = [None for i in range(num_all_layers)]\n",
    "caches = [None for i in range(num_all_layers)]\n",
    "# outs[0], caches[0] = affine_forward(X, self.params[\"W1\"], self.params[\"b1\"])\n",
    "for i in range(1, num_all_layers, 1):\n",
    "#     if i % num_loop == 1 :\n",
    "# #         outs[i], caches[i] = relu_forward(outs[i - 1])\n",
    "    if i % num_loop == 0:\n",
    "        outs[i], caches[i] = i // num_loop + 1, i // num_loop + 1\n",
    "print(outs)\n",
    "\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "# If test mode return early\n",
    "# if mode == 'test':\n",
    "#     return scores\n",
    "\n",
    "loss, grads = 0.0, {}\n",
    "############################################################################\n",
    "# TODO: Implement the backward pass for the fully-connected net. Store the #\n",
    "# loss in the loss variable and gradients in the grads dictionary. Compute #\n",
    "# data loss using softmax, and make sure that grads[k] holds the gradients #\n",
    "# for self.params[k]. Don't forget to add L2 regularization!               #\n",
    "#                                                                          #\n",
    "# When using batch/layer normalization, you don't need to regularize the scale   #\n",
    "# and shift parameters.                                                    #\n",
    "#                                                                          #\n",
    "# NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "# automated tests, make sure that your L2 regularization includes a factor #\n",
    "# of 0.5 to simplify the expression for the gradient.                      #\n",
    "############################################################################\n",
    "douts = [None for i in range(num_all_layers + 1)]\n",
    "# loss, douts[-1] = softmax_loss(scores, y)\n",
    "for i in range(num_all_layers - 1 , 0, -1):\n",
    "    if i % num_loop == 0:\n",
    "#         douts[i] = affine_backward(douts[i + 1], caches[i])\n",
    "        grads[f\"W{i // num_loop}\"], grads[f\"b{i // num_loop}\"] = i, i\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(8,0,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
